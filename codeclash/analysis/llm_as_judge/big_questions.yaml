# Incrementing this version will cause previous submissions to be re-evaluated
version: 4
system_prompt: |
  You need to answer three question

  ## Are edits motivated by logs?

  Are the edits of the main player file of the player directly motivated by problems discovered by reading the logs?

  We want to check if the edits of the main player file (`main.py`, `player.py`, `robot.js`, `warrior.red`, `robots/custom/`) are well motivated by the results of previous game logs.

  Special case: If there are no edits to the main player file, answer `True`.

  Reasons to answer `False` if there are edits to the main player file, but

  1. Player does not look at logs.
  2. Player reads some lines of the logs, but no clear failure mode is inferable. For example, the lines only state some game state, but it is not clear what is going wrong, for example because only the first lines of the game log are shown without showing the conclusion. Or the logs only show which player won but without much of a reason.
  3. Player runs an analysis script, but the analysis script does not return an actionable outcome or information that allows to infer it. For example, the analyais script only reports losses, but without clear attribution about what went wrong.
  4. A clear failure mode is uncovered in some of the logs or analyses, but the edits do not seem to be correlated to this failure mode.

  Answer `True` if ALL (!) of the following is true:

  1. A failure mode can be inferred with the help of reading the logs or analysis scripts evaluating the logs. Note that the failuire mode needs not be spelled out in any of the action outputs. It is enough that there is enough information to infer a failure mode based on basic reasoning.
  2. The edit is directly related to this failure mode. It is ok if some minor parts of the edit are unrelated.

  The logs can be either from a game that the player simulates it self, or from the previous round, but it must be a meaningful game log.

  Here are some example of real failure modes:

  - The snake that the player is controlling runs out of food (so we need to more aggressively search for food)
  - Our bot runs against a wall (so we change that)
  - Our race car does not move for several turns (so we fix some movement related bug)
  - Our warrior's missiles do not hit the enemy (so we improve something about the aim)
  - Our code times out (so we improve efficiency)

  Here are some example of not real failure modes:

  - Player 1 won 99% of the rounds (why?)
  - Player 2 is better most of the time (why?)
  - Player 1 is the last bot standing and is therefore the winner (does not explain why player 2 lost)

  ## Are edits motivated

  Can the goal of the specific edits on the main player file be motivated by any output of previous actions?
  If you answered True to the previous question, answer True here as well.
  However, you can also answer True here, if one or more of the following is true:

  1. The player wrote a meaningful test that revealed a problem (or a way to improve) and then performed the corresponding edit
  2. The player wrote a meaningful analysis script that revealed a problem (or a way to improve) and then performed the corresponding edit
  3. The player ran some test games that revealed a problem (or a way to improve) and then performed the corresponding edit
  4. The player made some changes, and then ran test games against the previous version and verified that the changes improved the performance, i.e., had a higher win rate.

  However, if for 1. and 2. the test or analysis script gives a recommendation that's not
  corroborated by the actual code of the analysis or test file, or by its respective output,
  answer False. This for example applies to static messages in the analysis or test file.
  If you do not see the output or the code of the analysis or test file, answer False as well.

  Just stating a low win rate is not a sufficient motivation.
  Remember: This is about the _specific_ edits being motivated, not just any edits.

  Here problem can mean anything from a bug in the code, to something that can be improved otherwise.

  You should answer False, if the edits seemed unrelated to any output of previous actions before the relevant
  edit actions.

  ## Are edits tested with simulations?

  After edits to the main player file have been made, does the player simulate games by playing the game against previous versions of itself, or against example players, etc.
  Only if we are performing a fix or an improvement that can be validated without an opponent (e.g., avoiding collisions with the wall in a car chase game), does a simulated game with only one player (the latest version) count.

  In order to answer `True`, a real game has to be played. If there is an opponent, the new version has to win (or have a good win rate).

  Note: If the game failed to run, or showed that the new version was clearly worse than the previous version, answer False.
  Note: Unit tests do NOT (!) count as a simulated game.

  Special case:

  1. If no edits to the main player file have been made, answer `True`

  ## Are edits validated with unittests?

  Are the final edits covered by specific unittests that test the new or modified behavior?

  Special case: If there are no significant changes that could be validated, answer True.

  Running the game to get a win rate does not count as a unittest, because it does not specifically validate specific changes.

  Running unittests that are unrelated to the changes does not count either.

  Answer `True`, if the unittests cover (some of) the new behavior. They do not have to be painfully complete or handle every special case, but they should test the core change that has been made.

  You can also count tests that only print output (but do not have assert statements) as unit tests,
  if they essentially print the expected output of the new or modified behavior and can therefore be
  used to validate the new or modified behavior.

  Note: if the tests did not run, or showed that the new version was broken, answer False.

  ## Categorize the kind of edits made

  Categorize the final edits to the main file into one of the following categories.
  Ignore comments or documentation.
  You can only select ONE (!) category. Choose the one that describes the changes best.

  1. `tweak`: Logic is left unchanged, but we do change some parameters.
  2. `fix`: Small, targeted change with the intent to fix broken behavior.
  3. `feature`: Significant new behavior is added, mostly extending the existing code
  4. `change`: We significantly change the behavior by rewriting significant logic of the code
  5. `none`: No change in behavior. Only comments, documentation, refactoring was performed.

  ## Output format

  Answer in the json format specified. Reasoning corresponds to your explanation for your answer.
instance_prompt: |
  Here is your input file:

  {{ trajectory_message_str }}
model:
  model_name: gpt-5
  model_kwargs:
    reasoning_effort: high
